add option to save the current state to continue some other time. this should include everything. where i am in the loops all the settings, 100% save and continue later. should be a different than just saving the settings that already exists.



if the review loop is set to 0 warn the user about it when he click start, he may choose to not start yet to change this or have 0 review (simular to how putting 0 questions works)

when i reach the limit with gemini i get this [15:48:21] [DEB] Final task file content: # Tasks YOLO mode is enabled. All tool calls will be automatically approved. Loaded cached credentials. Error when talking to Gemini API Full report available at: C:\Users\shlob\AppData\Local\Temp\gemini-client-error-Turn.run-sendMessageStream-2026-01-15T13-48-21-578Z.json TerminalQuotaError: You have exhausted your capacity on this model. Your quota will reset after 21h55m25s. at classifyGoogleError (file:///C:/Users/shlob/AppData/Roaming/npm/node_modules/@google/gemini-cli/node_modules/@g...
[15:48:21] [DEB] Single task execution result: {'task_completed': False, 'all_tasks_done': True, 'iteration': 0, 'stopped_early': False}. this is something i need to be able to catch at any phase and let the user know and let him switch to a different llm

when i reach the limit in claude i get something like this in a terminal: "PS C:\Users\shlob> claude -p "<prompt>"
You've hit your limit Â· resets 4pm (Asia/Jerusalem)". ofc it's important to recognize in any time zone so try and detect the You've hit your limit when using claude. and let the user know and let him switch to a different llm

we have to add another review option of UI/UX (besides the debugging and all the rest)

we need to make sure there are these flag files for each step (like if all tasks complete exit the main loop) if the llm found nothing wording in the debugging phase he should output somewhere all clear or something so we can skip the next iteration if all the debuggers passed no problem.

make sure we are dealing with cases where i launch the program again, meaning everything is reset. make sure questions json is empty and so on.

where the original project description was where the user entered the description after the question phase we should generate a full description taking into account the questions and display that there. there should also be the checklist of tasks that we are going through which should be marked as the ai get's through these tasks so the user can see visual progress

I should be able to change what llm is on what stage while it's running so if i want to change the debugger to codex for some reason mid run i should be able to do so

in the output log there is an option to filter but when i click it after a bunch of logs are generated it is not filtering anything. meaning the logs that were there stay there

should the task planning happen in a loop at the start??? meaning should we allow the llm to iterate a few times and break down tasks further if needed?

once all tasks are complete i should be able to run another prompt. currently if there is a bug or it finishes all buttons and inputs are disabled

enforcing the .md format and less than 1000 lines of code...... perhaps make a CLAUDE.md and an AGENTS.md and a GEMINI.md files at the start of the program with the rules? we can have defaults and allow the user to edit them for this project before the main loop starts.

if the debuggers change anything they should also update the recent changes

With UI programs we have the issue of the agent running it and then waiting for it to quit sometimes? does this mean we need user input?

recent changes should NOT be reset after each iteration instead the recent changes should not be over 500 lines long. this should be handled by the code to start erasing old lines to keep it at 500 lines

need an option in the UI to add things to the list of what needs to be done

should the questions.json be updated after the user inputs his answers?

is it clear to the reviewers in the prompt we are sending them to leave the review file empty if there are no complaints? i keep seeing in the review file "all good nothing to fix" instead of keeping it empty

why does it feel like everything is running slow? is all working on timeouts? or as soon as an agent is done it moves on to the next stage?

if a review loop finishes with all the review files empty we don't need to run the loop again

I mean long term we are going to want to not have to be stuck on one LLM for reviewing the code. i want to be able to have more than one review loop but have multiple llms do it (like claude for checking and codex for fixing and after that gemini for checking and claude for fixing) this needs to be easily adjustable in the UI

Long term we need to add support for all LLM's and models that people use and be able to use API keys and stuff like that.

review should start with general review and then go into specifics


I should be able to continue if the workflow is complete either by just saying keep running for x iterations and it will skip the whole question part or by giving it a new feature.

if it's a UI thingy is there a way to extract screenshots from the UI to debug

should we have the LLM decide what debuggers should run before they run?

should we have a more explicit way for agents to add tasks like have a dedicated call to the llm to check it out? meaning at the end of each full iteration have a look and see if any new tasks come up?